"""
ä¸»ç¨‹åº - LangChain Lite ç‰ˆæœ¬

ä½¿ç”¨ LangChain Chain ç»„ç»‡ LLM è°ƒç”¨ï¼Œå®Œå…¨ä¿ç•™åŸå§‹ workflow
"""

import os
os.environ["JINA_API_KEY"] = "jina_518b9cb292b249139bedce5123349109HnqXMjmaY94laLNX3J50eXfmd9E5"

import json
import sys
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from workflow.debate_workflow_lc import run_debate_workflow_lc
import config


def process_single_claim(claim: str, rounds: int = 3, ground_truth: str = None):
    """
    å¤„ç†å•ä¸ªclaim

    Args:
        claim: è¦æ ¸æŸ¥çš„claim
        rounds: è¾©è®ºè½®æ¬¡
        ground_truth: æ•°æ®é›†ä¸­çš„çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰

    Returns:
        åˆ¤å†³ç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"LangChain Lite äº‹å®æ ¸æŸ¥ç³»ç»Ÿ")
    print(f"ï¼ˆä¿ç•™åŸå§‹ workflowï¼Œä»…ç”¨ Chain ç»„ç»‡ LLM è°ƒç”¨ï¼‰")
    print(f"{'='*80}\n")
    print(f"Claim: {claim}\n")

    result = run_debate_workflow_lc(claim, max_rounds=rounds)

    # è®¾ç½® ground_truth
    if ground_truth:
        result["complete_log"]["ground_truth"] = ground_truth
        result["complete_log"]["evaluation"]["ground_truth"] = ground_truth
        result["complete_log"]["evaluation"]["correct"] = (
            result["verdict"]["decision"] == ground_truth
        )

    # ä¿å­˜ç»“æœ
    output_dir = Path("output/output_1_1")
    output_dir.mkdir(exist_ok=True)

    # ä¿å­˜è®ºè¾©å›¾
    with open(output_dir / "argumentation_graph_lc_lite.json", "w", encoding="utf-8") as f:
        json.dump(result["arg_graph_data"], f, ensure_ascii=False, indent=2, default=str)

    # ä¿å­˜åˆ¤å†³
    with open(output_dir / "verdict_lc_lite.json", "w", encoding="utf-8") as f:
        json.dump(result["verdict"], f, ensure_ascii=False, indent=2, default=str)

    # ä¿å­˜å®Œæ•´æ—¥å¿—
    with open(output_dir / "complete_log.json", "w", encoding="utf-8") as f:
        json.dump(result["complete_log"], f, ensure_ascii=False, indent=2, default=str)

    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ° output/ ç›®å½•")
    print(f"  - argumentation_graph_lc_lite.json")
    print(f"  - verdict_lc_lite.json")
    print(f"  - complete_log.json")

    return result


def process_dataset(dataset_path: str, output_path: str, max_samples: int = None):
    """
    æ‰¹é‡å¤„ç†æ•°æ®é›†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        max_samples: æœ€å¤§å¤„ç†æ•°é‡

    Returns:
        ç»“æœåˆ—è¡¨
    """
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    if max_samples:
        dataset = dataset[:max_samples]

    print(f"åŠ è½½äº† {len(dataset)} æ¡æ•°æ®\n")

    # åˆ›å»ºæ—¥å¿—ç›®å½•
    output_dir = Path("output/output_1_1")
    output_dir.mkdir(exist_ok=True)
    logs_dir = output_dir / "logs"
    logs_dir.mkdir(exist_ok=True)

    # è¿›åº¦æ–‡ä»¶è·¯å¾„
    progress_file = output_dir / "progress.json"

    # å°è¯•åŠ è½½å·²æœ‰ç»“æœå’Œè¿›åº¦
    results = []
    processed_indices = set()

    if progress_file.exists():
        try:
            with open(progress_file, "r", encoding="utf-8") as f:
                progress_data = json.load(f)
                processed_indices = set(progress_data.get("processed_indices", []))
                print(f"æ£€æµ‹åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå·²å¤„ç† {len(processed_indices)} æ¡æ•°æ®")
                print(f"   å°†ä»ç¬¬ {len(processed_indices)+1} æ¡ç»§ç»­...\n")
        except Exception as e:
            print(f"âš  æ— æ³•è¯»å–è¿›åº¦æ–‡ä»¶: {e}")

    # åŠ è½½å·²æœ‰ç»“æœ
    if Path(output_path).exists():
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                results = json.load(f)
                print(f"ğŸ“‚ åŠ è½½äº† {len(results)} æ¡å·²æœ‰ç»“æœ\n")
        except Exception as e:
            print(f"âš  æ— æ³•è¯»å–ç»“æœæ–‡ä»¶: {e}")
            results = []

    for i, item in enumerate(dataset):
        # è·³è¿‡å·²å¤„ç†çš„æ•°æ®
        if i in processed_indices:
            print(f"â­ï¸  è·³è¿‡ç¬¬ {i+1}/{len(dataset)} æ¡ï¼ˆå·²å¤„ç†ï¼‰")
            continue

        print(f"\n{'#'*70}")
        print(f"å¤„ç†ç¬¬ {i+1}/{len(dataset)} æ¡")
        print(f"{'#'*70}")

        try:
            result = run_debate_workflow_lc(item["claim"], max_rounds=3)

            ground_truth = item.get("verdict")

            # è®¾ç½® ground_truth åˆ° complete_log
            if ground_truth:
                result["complete_log"]["ground_truth"] = ground_truth
                result["complete_log"]["evaluation"]["ground_truth"] = ground_truth
                result["complete_log"]["evaluation"]["correct"] = (
                    result["verdict"]["decision"] == ground_truth
                )

            # ä¿å­˜å•ä¸ª claim çš„å®Œæ•´æ—¥å¿—
            log_filename = f"log_{i+1:03d}.json"
            with open(logs_dir / log_filename, "w", encoding="utf-8") as f:
                json.dump(result["complete_log"], f, ensure_ascii=False, indent=2, default=str)

            results.append({
                "index": i,
                "claim": item["claim"],
                "predicted": result["verdict"].get("decision"),
                "ground_truth": ground_truth,
                "confidence": result["verdict"].get("confidence"),
                "correct": result["verdict"].get("decision") == ground_truth
            })

            # ç«‹å³ä¿å­˜ç»“æœæ‘˜è¦
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            # æ›´æ–°è¿›åº¦æ–‡ä»¶
            processed_indices.add(i)
            with open(progress_file, "w", encoding="utf-8") as f:
                json.dump({
                    "processed_indices": sorted(list(processed_indices)),
                    "total": len(dataset),
                    "last_updated": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)

            print(f"âœ“ å®Œæ•´æ—¥å¿—å·²ä¿å­˜: {log_filename}")
            print(f"âœ“ è¿›åº¦å·²æ›´æ–°: {len(processed_indices)}/{len(dataset)}")

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

            # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜ä¸€æ¡é”™è¯¯è®°å½•
            results.append({
                "index": i,
                "claim": item["claim"],
                "predicted": None,
                "ground_truth": item.get("verdict"),
                "confidence": None,
                "correct": None,
                "error": str(e)
            })

            # ä¿å­˜ç»“æœå’Œè¿›åº¦
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            processed_indices.add(i)
            with open(progress_file, "w", encoding="utf-8") as f:
                json.dump({
                    "processed_indices": sorted(list(processed_indices)),
                    "total": len(dataset),
                    "last_updated": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)

            continue

    # ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡æˆåŠŸçš„ï¼‰
    valid_results = [r for r in results if r.get("predicted") is not None]
    correct = sum(1 for r in valid_results if r.get("correct"))
    total = len([r for r in valid_results if r.get("correct") is not None])
    accuracy = correct / total if total > 0 else 0

    print(f"\n\n{'='*70}")
    print(f"è¯„ä¼°å®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»æ•°: {total}")
    print(f"æ­£ç¡®: {correct}")
    print(f"å‡†ç¡®ç‡: {accuracy:.2%}")

    # ä¿å­˜ç»Ÿè®¡
    stats = {
        "total": total,
        "correct": correct,
        "accuracy": accuracy,
        "processed": len(processed_indices),
        "failed": len(results) - total,
        "results": results
    }

    with open(output_path.replace('.json', '_stats.json'), "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LangChain Lite è¾©è®ºå¼äº‹å®æ ¸æŸ¥")
    # parser.add_argument("--claim", type=str, default="è¶Šå—é€šè¿‡äººå·¥æ™ºèƒ½æ³•,åœ¨æœ€åç«‹æ³•ä¼šè®®ä¸­é€šè¿‡51é¡¹æ³•æ¡ˆ")
    parser.add_argument("--dataset", type=str, default="../data/dataset_part_8.json")
    parser.add_argument("--output", type=str, default="output/output_8/results_lc_lite.json")
    parser.add_argument("--max-samples", type=int, default=50)
    parser.add_argument("--rounds", type=int, default=3, help="è¾©è®ºè½®æ¬¡")

    args = parser.parse_args()

    # if args.claim:
    #     process_single_claim(args.claim, args.rounds)
    # elif args.dataset:
    process_dataset(args.dataset, args.output, args.max_samples)
    # else:
    #     # é»˜è®¤æµ‹è¯•
    #     test_claim = "æ¬§ç›Ÿè®¡åˆ’åœ¨2030å¹´å…¨é¢ç¦æ­¢é”€å”®ç‡ƒæ²¹è½¦ã€‚"
    #     process_single_claim(test_claim, rounds=2)