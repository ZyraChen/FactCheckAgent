"""
Debate Orchestrator - å¤šAgentè¾©è®ºç¼–æ’å™¨ (LangChainç‰ˆæœ¬)

è´Ÿè´£ç¼–æ’Pro, Con, Judgeä¸‰ä¸ªAgentçš„äº¤äº’
"""

import json
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import config
from llm.qwen_client import QwenClient
from tools.jina_search import JinaSearch
from core.evidence_pool import EvidencePool
from core.argumentation_graph import ArgumentationGraph
from tools.attack_detector import AttackDetector
from utils.models import Verdict

# å¯¼å…¥ LangChain agents å’Œ tools
from langchain_version.agents.pro_agent_lc import create_pro_agent, run_pro_agent
from langchain_version.agents.con_agent_lc import create_con_agent, run_con_agent
from langchain_version.agents.judge_agent_lc import create_judge_agent, run_judge_agent
from langchain_version.tools.search_tool import SearchTool
from langchain_version.tools.evidence_pool_tool import EvidencePoolTool
from langchain_version.tools.argument_graph_tool import ArgumentGraphTool


class DebateOrchestrator:
    """
    è¾©è®ºç¼–æ’å™¨ - LangChain å¤šAgentç‰ˆæœ¬

    èŒè´£:
    1. åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ (LLM, Tools, Agents)
    2. ç¼–æ’å¤šè½®è¾©è®ºæµç¨‹
    3. åè°ƒPro, Con, Judgeä¸‰ä¸ªAgent
    4. ç®¡ç†å…±äº«çŠ¶æ€ (è¯æ®æ± , è®ºè¾©å›¾)
    """

    def __init__(self, claim: str, max_rounds: int = 3):
        """
        åˆå§‹åŒ–ç¼–æ’å™¨

        Args:
            claim: è¦æ ¸æŸ¥çš„claim
            max_rounds: æœ€å¤§è¾©è®ºè½®æ¬¡
        """
        self.claim = claim
        self.max_rounds = max_rounds

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        print(f"\n{'='*80}")
        print(f"åˆå§‹åŒ–LangChainå¤šAgentè¾©è®ºç³»ç»Ÿ")
        print(f"{'='*80}\n")

        self.llm = QwenClient(config.DASHSCOPE_API_KEY)
        self.jina = JinaSearch(config.JINA_API_KEY)
        self.evidence_pool = EvidencePool()
        self.arg_graph = ArgumentationGraph(claim)
        self.attack_detector = AttackDetector(self.llm)

        # åˆå§‹åŒ– LangChain Tools
        print("åˆ›å»º LangChain Tools...")
        self.search_tool = SearchTool(
            jina_client=self.jina,
            evidence_pool=self.evidence_pool,
            arg_graph=self.arg_graph
        )
        self.evidence_pool_tool = EvidencePoolTool(evidence_pool=self.evidence_pool)
        self.arg_graph_tool = ArgumentGraphTool(arg_graph=self.arg_graph)

        print("âœ“ ç»„ä»¶åˆå§‹åŒ–å®Œæˆ\n")

    def run_debate(self) -> dict:
        """
        è¿è¡Œå®Œæ•´çš„è¾©è®ºæµç¨‹

        Returns:
            åŒ…å«åˆ¤å†³ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        print(f"\n{'='*80}")
        print(f"å¼€å§‹è¾©è®º: {self.claim}")
        print(f"{'='*80}\n")

        # å¤šè½®è¾©è®º
        for round_num in range(1, self.max_rounds + 1):
            print(f"\n{'='*70}")
            print(f"ç¬¬ {round_num}/{self.max_rounds} è½®")
            print(f"{'='*70}\n")

            # åˆ›å»ºæœ¬è½®çš„ Agents
            print(f"[åˆ›å»ºAgents]")
            pro_agent = create_pro_agent(
                claim=self.claim,
                round_num=round_num,
                llm_client=self.llm,
                search_tool=self.search_tool,
                evidence_pool_tool=self.evidence_pool_tool
            )

            con_agent = create_con_agent(
                claim=self.claim,
                round_num=round_num,
                llm_client=self.llm,
                search_tool=self.search_tool,
                evidence_pool_tool=self.evidence_pool_tool
            )

            # Pro Agent è¡ŒåŠ¨
            print(f"\n[Pro Agent è¡ŒåŠ¨]")
            con_evidences = self.evidence_pool.get_by_agent("con")
            con_summary = self._summarize_evidences(con_evidences[-3:]) if con_evidences else ""

            try:
                pro_result = run_pro_agent(
                    agent_executor=pro_agent,
                    claim=self.claim,
                    round_num=round_num,
                    opponent_evidences_summary=con_summary
                )
                print(f"Pro Agent å®Œæˆ: {pro_result[:200]}...")
            except Exception as e:
                print(f"âš  Pro Agent æ‰§è¡Œå¤±è´¥: {e}")

            # Con Agent è¡ŒåŠ¨
            print(f"\n[Con Agent è¡ŒåŠ¨]")
            pro_evidences = self.evidence_pool.get_by_agent("pro")
            pro_summary = self._summarize_evidences(pro_evidences[-3:]) if pro_evidences else ""

            try:
                con_result = run_con_agent(
                    agent_executor=con_agent,
                    claim=self.claim,
                    round_num=round_num,
                    opponent_evidences_summary=pro_summary
                )
                print(f"Con Agent å®Œæˆ: {con_result[:200]}...")
            except Exception as e:
                print(f"âš  Con Agent æ‰§è¡Œå¤±è´¥: {e}")

            # æ£€æµ‹æ”»å‡»å…³ç³»
            print(f"\n[æ”»å‡»æ£€æµ‹]")
            new_attacks = self.attack_detector.detect_attacks_for_round(self.arg_graph, round_num)
            self.arg_graph.add_attacks(new_attacks)
            print(f"âœ“ æ–°å¢ {len(new_attacks)} ä¸ªæ”»å‡»è¾¹")

            # æœ¬è½®ç»Ÿè®¡
            stats = self.evidence_pool.get_statistics()
            print(f"\n[æœ¬è½®ç»Ÿè®¡] Pro:{stats['pro']}ä¸ª, Con:{stats['con']}ä¸ª, æ€»è®¡:{stats['total']}ä¸ªè¯æ®")

        # Judge åˆ¤å†³
        print(f"\n{'='*80}")
        print("Judge Agent åˆ¤å†³")
        print(f"{'='*80}\n")

        judge_agent = create_judge_agent(
            claim=self.claim,
            llm_client=self.llm,
            arg_graph_tool=self.arg_graph_tool,
            evidence_pool_tool=self.evidence_pool_tool
        )

        try:
            judge_result = run_judge_agent(
                agent_executor=judge_agent,
                claim=self.claim
            )
            print(f"\nJudge åˆ¤å†³ç»“æœ:\n{judge_result}")

            # è§£æåˆ¤å†³ç»“æœ (å‡è®¾æ˜¯ JSON æ ¼å¼)
            verdict_data = self._parse_judge_output(judge_result)

        except Exception as e:
            print(f"âš  Judge Agent æ‰§è¡Œå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•åˆ¤å†³
            verdict_data = {
                "decision": "NEI",
                "confidence": 0.3,
                "reasoning": f"Judge Agent æ‰§è¡Œå¤±è´¥: {e}",
                "key_evidence_ids": [],
                "support_strength": 0.0,
                "refute_strength": 0.0
            }

        # æ„å»ºæœ€ç»ˆç»“æœ
        verdict = Verdict(
            decision=verdict_data.get("decision", "NEI"),
            confidence=verdict_data.get("confidence", 0.5),
            reasoning=verdict_data.get("reasoning", ""),
            key_evidence_ids=verdict_data.get("key_evidence_ids", []),
            accepted_evidence_ids=list(self.arg_graph.compute_grounded_extension()),
            pro_strength=verdict_data.get("support_strength", 0.0),
            con_strength=verdict_data.get("refute_strength", 0.0),
            total_evidences=len(self.arg_graph.evidence_nodes),
            accepted_evidences=len(self.arg_graph.compute_grounded_extension())
        )

        # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
        self._print_final_report(verdict)

        return {
            "claim": self.claim,
            "verdict": verdict.model_dump(),
            "evidence_pool_stats": self.evidence_pool.get_statistics(),
            "arg_graph_data": self.arg_graph.to_dict()
        }

    def _summarize_evidences(self, evidences) -> str:
        """ç”Ÿæˆè¯æ®æ‘˜è¦"""
        if not evidences:
            return ""

        summary = []
        for ev in evidences:
            summary.append(f"- [{ev.source}] {ev.content[:100]}...")

        return "\n".join(summary)

    def _parse_judge_output(self, judge_output: str) -> dict:
        """è§£æ Judge Agent çš„è¾“å‡º"""
        try:
            # å°è¯•ä»è¾“å‡ºä¸­æå– JSON
            import re
            json_match = re.search(r'\{[^}]+\}', judge_output, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    "decision": "NEI",
                    "confidence": 0.5,
                    "reasoning": judge_output,
                    "key_evidence_ids": [],
                    "support_strength": 0.0,
                    "refute_strength": 0.0
                }
        except:
            return {
                "decision": "NEI",
                "confidence": 0.5,
                "reasoning": judge_output,
                "key_evidence_ids": [],
                "support_strength": 0.0,
                "refute_strength": 0.0
            }

    def _print_final_report(self, verdict: Verdict):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        print(f"\n\n{'='*80}")
        print("ğŸ“Š æœ€ç»ˆåˆ¤å†³")
        print(f"{'='*80}\n")
        print(f"Claim: {self.claim}\n")
        print(f"åˆ¤å†³: {verdict.decision}")
        print(f"ç½®ä¿¡åº¦: {verdict.confidence:.2%}")
        print(f"\næ¨ç†è¿‡ç¨‹:")
        print("-" * 80)
        print(verdict.reasoning)
        print(f"\nç»Ÿè®¡:")
        print(f"- æ€»è¯æ®: {verdict.total_evidences}")
        print(f"- è¢«æ¥å—: {verdict.accepted_evidences}")
        print(f"- æ”¯æŒå¼ºåº¦: {verdict.pro_strength:.2f}")
        print(f"- åå¯¹å¼ºåº¦: {verdict.con_strength:.2f}")


def run_langchain_debate(claim: str, max_rounds: int = 3) -> dict:
    """
    è¿è¡Œ LangChain å¤šAgentè¾©è®º (ä¾¿æ·å‡½æ•°)

    Args:
        claim: è¦æ ¸æŸ¥çš„claim
        max_rounds: æœ€å¤§è¾©è®ºè½®æ¬¡

    Returns:
        è¾©è®ºç»“æœå­—å…¸
    """
    orchestrator = DebateOrchestrator(claim=claim, max_rounds=max_rounds)
    return orchestrator.run_debate()


if __name__ == "__main__":
    # æµ‹è¯•
    test_claim = "æ¬§ç›Ÿè®¡åˆ’åœ¨2030å¹´å…¨é¢ç¦æ­¢é”€å”®ç‡ƒæ²¹è½¦ã€‚"
    result = run_langchain_debate(test_claim, max_rounds=2)

    print(f"\n\n{'='*80}")
    print("æµ‹è¯•å®Œæˆ")
    print(f"{'='*80}")
    print(f"åˆ¤å†³: {result['verdict']['decision']}")
    print(f"ç½®ä¿¡åº¦: {result['verdict']['confidence']:.2f}")
