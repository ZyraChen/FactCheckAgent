"""
ç®€åŒ–ç‰ˆLLM Baselineæµ‹è¯•è„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰
åŠŸèƒ½ï¼š
1. æ£€æµ‹ Verdict Accuracyï¼ˆåˆ¤å†³å‡†ç¡®åº¦ï¼‰
2. è¿”å› LLMçš„åˆ¤å†³ç†ç”±ï¼ˆjustificationï¼‰
3. è¿”å› LLMæœç´¢åˆ°çš„è¯æ®ï¼ˆevidence_sourcesï¼‰
4. ä»¥ç»“æ„åŒ–JSONæ ¼å¼ä¿å­˜

ä¿®å¤é—®é¢˜ï¼š
1. promptå’Œä»£ç å­—æ®µåä¸åŒ¹é…ï¼ˆjustification vs reasoningï¼‰
2. promptçš„JSONæ ¼å¼é”™è¯¯
3. æ·»åŠ å¯¹LLMè¿”å›çš„evidence_sourcesçš„å¤„ç†
"""

import json
import time
import re
from typing import List, Dict, Set
from collections import defaultdict
import openai


class QwenPlus:
    """é€šä¹‰åƒé—®LLMï¼ˆå¸¦æœç´¢åŠŸèƒ½ï¼‰"""

    def __init__(self, api_key):
        self.model = "qwen-plus-latest"
        self.llm = openai.OpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

    def _cons_kwargs(self, messages: list[dict]) -> dict:
        kwargs = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.3,
            "timeout": 60,
        }
        return kwargs

    def completion(self, messages: list[dict], enable_thinking=False, return_json=False, enable_search=False) -> str:
        """è°ƒç”¨LLM completion"""
        response_format = {"type": "json_object"} if not enable_thinking and return_json else {"type": "text"}
        extra_body = {}

        if enable_search:
            extra_body = {
                "enable_thinking": enable_thinking,
                "enable_search": True,
                "search_options": {
                    "forced_search": True,
                    "search_strategy": "max"
                }
            }

        try:
            rsp = self.llm.chat.completions.create(
                **self._cons_kwargs(messages),
                extra_body=extra_body if enable_search else None,
                response_format=response_format
            )
        except openai.RateLimitError as e:
            print("    âš ï¸  APIè¯·æ±‚è¶…è¿‡é™åˆ¶ï¼Œç­‰å¾…60ç§’...")
            time.sleep(60)
            rsp = self.llm.chat.completions.create(
                **self._cons_kwargs(messages),
                extra_body=extra_body if enable_search else None,
                response_format=response_format
            )
        except openai.APITimeoutError as e:
            print("    âš ï¸  APIè¯·æ±‚è¶…æ—¶ï¼Œç­‰å¾…60ç§’...")
            time.sleep(60)
            rsp = self.llm.chat.completions.create(
                **self._cons_kwargs(messages),
                extra_body=extra_body if enable_search else None,
                response_format=response_format
            )

        return rsp.choices[0].message.content


class EvidenceExtractor:
    """è¯æ®æå–å™¨ - ä»LLMçš„justificationä¸­æå–å¼•ç”¨çš„è¯æ®"""

    @staticmethod
    def extract_evidence_from_text(text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–è¯æ®å¼•ç”¨ï¼ˆä½œä¸ºå¤‡ç”¨æ–¹æ³•ï¼‰
        å½“LLMæ²¡æœ‰è¿”å›ç»“æ„åŒ–çš„evidence_sourcesæ—¶ä½¿ç”¨
        """
        evidence_list = []

        if not text:
            return evidence_list

        # 1. æå–URL
        urls = re.findall(r'https?://[^\s\)\],]+', text)
        for url in urls:
            if url not in evidence_list:
                evidence_list.append(url)

        # 2. æå–å¼•ç”¨çš„æ¥æºåç§°
        citation_patterns = [
            r'[Aa]ccording to ([^,\.;]+)',
            r'([A-Z][a-zA-Z\s&\.]+(?:University|Institute|Bureau|Department|Bank|Agency|Organization|Commission|Post|Times|Journal|News|Guard|Press|Report|Survey|Index|Board))\s+(?:reported|stated|found|showed|indicated|confirmed|says|said)',
            r'[Bb]ased on ([^,\.;]+(?:data|report|survey|study|analysis))',
            r"([A-Z][a-zA-Z\s&]+(?:University|Institute|Bureau|Department|Bank|Agency|Organization|Commission|Board))'s",
            r'[Tt]he ([A-Z][a-zA-Z\s&]+(?:Report|Survey|Index|Study))',
            r'[Aa]ccording to (?:the )?([A-Z]{2,})',
            r'([A-Z]{2,})\s+(?:data|report|survey|found|stated)',
        ]

        for pattern in citation_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) > 2 and clean_match not in evidence_list:
                    evidence_list.append(clean_match)

        # 3. æå–å…³é”®æ•°å­—å’Œç™¾åˆ†æ¯”
        percentages = re.findall(r'\d+(?:\.\d+)?%', text)
        for pct in percentages:
            if pct not in evidence_list:
                evidence_list.append(pct)

        return evidence_list


class VerdictTester:
    """Verdictå‡†ç¡®åº¦æµ‹è¯•å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, api_key: str, dataset_path: str, enable_search: bool = True):
        self.llm = QwenPlus(api_key)
        self.dataset = self.load_dataset(dataset_path)
        self.results = []
        self.enable_search = enable_search
        self.evidence_extractor = EvidenceExtractor()

    def load_dataset(self, path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®é›†"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def test_single_claim(self, item: Dict, index: int) -> Dict:
        """æµ‹è¯•å•ä¸ªclaim"""
        claim = item['claim']
        ground_truth_verdict = item['verdict']

        print(f"\n{'=' * 80}")
        print(f"[{index}] Claim: {claim[:100]}...")
        print(f"Ground Truth Verdict: {ground_truth_verdict}")

        try:
            # ä¿®å¤åçš„prompt - å­—æ®µåç»Ÿä¸€ï¼ŒJSONæ ¼å¼æ­£ç¡®
            prompt = f"""You are a professional fact-checker with access to search capabilities. Use web search to verify the following claim.

Claim: "{claim}"

CRITICAL REQUIREMENTS:
1. You must search for relevant, credible information about this claim
2. In your justification, you MUST cite specific sources:
   - You can use formats like: "According to [Source Name]..."
   - Include URLs in evidence sources
   - Mention the source organization/publication name
   - Cite specific numbers, dates, and statistics

3. Provide your verdict as one of these exact terms:
   - "Supported" (if the claim is true based on evidence)
   - "Refuted" (if the claim is false based on evidence)
   - "Not Enough Evidence" (if you cannot find sufficient information)

4. Provide a complete justification (4-6 sentences with explicit source citations)

5. Provide less than 5 evidence sources you actually used, including:
   - content: Primary content of the evidence
   - url: 
       - ONLY include URLs that you ACTUALLY found during your search. 
       - DO NOT fabricate, guess, or construct URLs
       - DO NOT use common patterns like "https://example.com/..."
       - ONLY use URLs that appeared in your search results
   - credibility: "High" (government, academic, mainstream media) | "Medium" (industry reports, local news) | "Low" (social media, unverified)

GOOD EXAMPLE:
{{
   "claim": "å€ªè¡Œå†›ç›®å‰æ˜¯èš‚èšé›†å›¢çš„è‘£äº‹ã€‚",
    "verdict": "Refuted",
    "justification": "é”™è¯¯ï¼Œå…¬å¼€èµ„æ–™æ˜¾ç¤ºï¼Œå€ªè¡Œå†›é•¿æœŸåœ¨èš‚èšé›†å›¢æ‹…ä»»é¦–å¸­æŠ€æœ¯å®˜ã€èµ„æ·±å‰¯æ€»è£ä»¥åŠæŠ€æœ¯æˆ˜ç•¥å§”å‘˜ä¼šä¸»å¸­ç­‰ç®¡ç†å²—ä½ï¼Œå¹¶ä¸”è‡ª 2020 å¹´èµ·æ›¾æ‹…ä»»èš‚èšé›†å›¢æ‰§è¡Œè‘£äº‹ï¼ˆExecutive Directorï¼‰ã€‚ä½†åç»­è‘£äº‹ä¼šè°ƒæ•´ä¸­ï¼Œèš‚èšé›†å›¢å®˜ç½‘åŠå¤šå®¶åª’ä½“æŠ¥é“å‡æŒ‡å‡ºï¼šé¦–å¸­æŠ€æœ¯å®˜å€ªè¡Œå†›ä¸å†æ‹…ä»»èš‚èšé›†å›¢æ‰§è¡Œè‘£äº‹ï¼Œå…¶è‘£äº‹å¸­ä½ç”±é¦–å¸­è´¢åŠ¡å®˜éŸ©æ­†æ¯…æ¥ä»»ï¼Œå½“å‰èš‚èšé›†å›¢å®˜ç½‘çš„é¢†å¯¼å±‚é¡µé¢åˆ—å‡ºå€ªè¡Œå†›çš„å¤´è¡”ä¸ºâ€œèµ„æ·±å‰¯æ€»è£ã€æŠ€æœ¯æˆ˜ç•¥å§”å‘˜ä¼šä¸»å¸­â€ï¼Œä¸å†åˆ—å…¥è‘£äº‹ä¼šæˆå‘˜ï¼Œå› æ­¤å°†å…¶ç§°ä¸ºâ€œç›®å‰æ˜¯èš‚èšé›†å›¢çš„è‘£äº‹â€ä¸æœ€æ–°ç»“æ„ä¸ç¬¦ã€‚",
    "evidence_sources": [
      {{
        "content": "èš‚èšé›†å›¢(688688)å…¬å¸é«˜ç®¡ â€“ æ–°æµªè´¢ç»ï¼ˆè¯´æ˜å€ªè¡Œå†›è‡ª 2020 å¹´ 7 æœˆèµ·æ‹…ä»»èš‚èšé›†å›¢æ‰§è¡Œè‘£äº‹ã€è‡ª 2020 å¹´ 8 æœˆèµ·æ‹…ä»»é¦–å¸­æŠ€æœ¯å®˜ï¼Œä½†è¯¥ä¿¡æ¯ä¸»è¦åæ˜ çš„æ˜¯èµ·ä»»æ—¶é—´ä¸æ›¾ä»»èŒåŠ¡ã€‚ï¼‰",
        "url": "https://money.finance.sina.com.cn/corp/view/vCI_CorpManagerInfo.php?stockid=688688&Pcode=30501025&Name=%C4%DD%D0%D0%BE%FC",
        "credibility": "Medium"
      }},
      {{
        "content": "èš‚èšé›†å›¢è‘£äº‹ä¼šè°ƒæ•´ï¼šéŸ©æ­†æ¯…æ¥æ›¿å€ªè¡Œå†›å‡ºä»»æ‰§è¡Œè‘£äº‹ â€“ ç”µå•†è¡Œä¸šåª’ä½“æŠ¥é“æŒ‡å‡ºï¼Œæ ¹æ®èš‚èšé›†å›¢å®˜ç½‘æŠ«éœ²ï¼Œé¦–å¸­æŠ€æœ¯å®˜å€ªè¡Œå†›ä¸å†æ‹…ä»»å…¬å¸æ‰§è¡Œè‘£äº‹ï¼Œç”±é¦–å¸­è´¢åŠ¡å®˜éŸ©æ­†æ¯…æ¥ä»»ï¼Œå…¶åè‘£äº‹ä¼šæˆå‘˜åå•ä¸­ä¸å†åŒ…å«å€ªè¡Œå†›ã€‚",
        "url": "https://www.pai.com.cn/207021.html",
        "credibility": "Medium"
      }},
      {{
        "content": "Xingjun NI â€“ Senior Vice President, Chairman of Technology Strategy Committee â€“ Ant Group å®˜æ–¹è‹±æ–‡é¡µé¢ï¼ˆä»‹ç»å€ªè¡Œå†›ç›®å‰çš„èŒåŠ¡ä¸ºèš‚èšé›†å›¢èµ„æ·±å‰¯æ€»è£ã€æŠ€æœ¯æˆ˜ç•¥å§”å‘˜ä¼šä¸»å¸­ï¼ŒåŒæ—¶æ‹…ä»» OceanBase è‘£äº‹é•¿ï¼Œæ²¡æœ‰å°†å…¶åˆ—ä¸º Ant Group è‘£äº‹ä¼šæˆå‘˜ã€‚ï¼‰",
        "url": "https://www.antgroup.com/en/about/leadership/xingjun-ni",
        "credibility": "High"
      }}
    ],
}}

Respond ONLY with a valid JSON object in this exact format, The language of the response content must be consistent with the claim.:
{{
  "claim": "the original claim text",
  "verdict": "Supported" | "Refuted" | "Not Enough Evidence",
  "justification": "Your detailed reasoning with source citations",
  "confidence": "High" | "Medium" | "Low",
  "evidence_sources": [
    {{
      "content": "Evidence description",
      "url": "Real URL",
      "credibility": "High" | "Medium" | "Low"
    }}
  ]
}}

Do not include any text outside the JSON object."""

            messages = [{"role": "user", "content": prompt}]

            print(f"  ğŸ” æ­£åœ¨è®©LLMåˆ†æ...")
            response = self.llm.completion(messages, return_json=True, enable_search=self.enable_search)

            # æ‰“å°åŸå§‹å“åº”ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"\n  ã€åŸå§‹å“åº”ã€‘")
            print(f"  {response[:300]}...")

            # è§£æå“åº” - ä¿®å¤å­—æ®µå
            try:
                response_json = json.loads(response)
                llm_verdict = response_json.get('verdict', 'Not Enough Evidence')
                llm_justification = response_json.get('justification', '')  # ä¿®å¤ï¼šä»justificationè·å–
                llm_confidence = response_json.get('confidence', 'Unknown')
                llm_evidence_sources = response_json.get('evidence_sources', [])  # æ–°å¢ï¼šè·å–evidence_sources

                print(f"\n  ã€è§£ææˆåŠŸã€‘")
                print(f"  - verdict: {llm_verdict}")
                print(f"  - justificationé•¿åº¦: {len(llm_justification)} å­—ç¬¦")
                print(f"  - evidence_sourcesæ•°é‡: {len(llm_evidence_sources)}")

            except json.JSONDecodeError as e:
                print(f"    âš ï¸  JSONè§£æå¤±è´¥: {e}")
                print(f"    åŸå§‹å“åº”: {response[:500]}")
                llm_verdict = 'Not Enough Evidence'
                llm_justification = response
                llm_confidence = 'Low'
                llm_evidence_sources = []

            print(f"\n  LLM Verdict: {llm_verdict}")
            print(f"  Confidence: {llm_confidence}")

            # åˆ¤æ–­verdictæ˜¯å¦åŒ¹é…
            verdict_match = (llm_verdict == ground_truth_verdict)
            print(f"  Verdict Match: {'âœ… æ­£ç¡®' if verdict_match else 'âŒ é”™è¯¯'}")

            if not verdict_match:
                print(f"    Expected: {ground_truth_verdict}")
                print(f"    Got: {llm_verdict}")

            # æ˜¾ç¤ºLLMè¿”å›çš„è¯æ®æ¥æº
            if llm_evidence_sources:
                print(f"\n  ğŸ“š LLMæä¾›çš„è¯æ®æ¥æº ({len(llm_evidence_sources)}æ¡):")
                for i, ev in enumerate(llm_evidence_sources[:3], 1):
                    content = ev.get('content', '')[:80]
                    url = ev.get('url', 'N/A')
                    cred = ev.get('credibility', 'Unknown')
                    print(f"    {i}. [{cred}] {content}...")
                    print(f"       URL: {url}")
            else:
                print(f"\n  âš ï¸  LLMæœªè¿”å›ç»“æ„åŒ–çš„evidence_sources")

                # å¤‡ç”¨æ–¹æ¡ˆï¼šä»justificationä¸­æå–è¯æ®
                extracted_evidence = self.evidence_extractor.extract_evidence_from_text(llm_justification)
                if extracted_evidence:
                    print(f"  ğŸ” ä»justificationä¸­æå–åˆ° {len(extracted_evidence)} æ¡è¯æ®å¼•ç”¨:")
                    for i, ev in enumerate(extracted_evidence[:5], 1):
                        ev_display = ev[:80] + ('...' if len(ev) > 80 else '')
                        print(f"    {i}. {ev_display}")

            # è¿”å›ç»“æ„åŒ–ç»“æœ
            return {
                'index': index,
                'claim': claim,

                # Ground Truth
                'ground_truth': {
                    'verdict': ground_truth_verdict,
                    'justification': item.get('justification', ''),
                    'evidence_sources': item.get('evidence_sources', [])
                },

                # LLM Response
                'llm_response': {
                    'verdict': llm_verdict,
                    'justification': llm_justification,  # ä¿®å¤ï¼šä½¿ç”¨justification
                    'confidence': llm_confidence,
                    'evidence_sources': llm_evidence_sources,  # æ–°å¢ï¼šLLMè¿”å›çš„è¯æ®æ¥æº
                },

                # Verdictè¯„ä¼°
                'verdict_evaluation': {
                    'is_correct': verdict_match,
                    'expected': ground_truth_verdict,
                    'predicted': llm_verdict
                },

                'success': True,
                'error': None
            }

        except Exception as e:
            print(f"  âœ— Error: {str(e)}")
            import traceback
            traceback.print_exc()

            return {
                'index': index,
                'claim': claim,
                'ground_truth': {
                    'verdict': ground_truth_verdict
                },
                'success': False,
                'error': str(e)
            }

    def test_dataset(self, max_items: int = None, start_index: int = 0):
        """æµ‹è¯•æ•°æ®é›†"""
        print(f"{'=' * 80}")
        print(f"Verdictå‡†ç¡®åº¦æµ‹è¯•ï¼ˆä¿®å¤ç‰ˆï¼‰")
        print(f"{'=' * 80}")
        print(f"æ•°æ®é›†å¤§å°: {len(self.dataset)}")
        print(f"LLMæ¨¡å‹: Qwen-Plus-Latest")
        print(f"æœç´¢åŠŸèƒ½: {'âœ… å·²å¼€å¯' if self.enable_search else 'âŒ æœªå¼€å¯'}")

        test_items = self.dataset[start_index:start_index + max_items] if max_items else self.dataset[start_index:]
        print(f"æµ‹è¯•æ•°é‡: {len(test_items)}\n")

        for i, item in enumerate(test_items, start=start_index):
            result = self.test_single_claim(item, i)
            self.results.append(result)

            # æ¯5ä¸ªæš‚åœä¸€ä¸‹ï¼Œé¿å…APIé™æµ
            if (i - start_index + 1) % 5 == 0:
                print(f"\nâ¸ï¸  å·²å¤„ç† {i - start_index + 1}/{len(test_items)}ï¼Œæš‚åœ3ç§’...")
                time.sleep(3)
            else:
                time.sleep(1)

        return self.results

    def calculate_accuracy(self) -> Dict:
        """è®¡ç®—å‡†ç¡®ç‡ç»Ÿè®¡"""
        successful = [r for r in self.results if r['success']]

        if not successful:
            return {'error': 'æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•'}

        total = len(successful)
        correct = sum(1 for r in successful if r['verdict_evaluation']['is_correct'])
        accuracy = correct / total

        # æŒ‰verdictåˆ†ç±»ç»Ÿè®¡
        by_verdict = defaultdict(lambda: {'total': 0, 'correct': 0})
        for r in successful:
            gt_verdict = r['ground_truth']['verdict']
            by_verdict[gt_verdict]['total'] += 1
            if r['verdict_evaluation']['is_correct']:
                by_verdict[gt_verdict]['correct'] += 1

        accuracy_by_verdict = {
            verdict: {
                'accuracy': stats['correct'] / stats['total'] if stats['total'] > 0 else 0.0,
                'correct': stats['correct'],
                'total': stats['total']
            }
            for verdict, stats in by_verdict.items()
        }

        # æ··æ·†çŸ©é˜µ
        confusion_matrix = defaultdict(lambda: defaultdict(int))
        for r in successful:
            gt = r['verdict_evaluation']['expected']
            pred = r['verdict_evaluation']['predicted']
            confusion_matrix[gt][pred] += 1

        return {
            'total_tests': len(self.results),
            'successful_tests': total,
            'failed_tests': len(self.results) - total,
            'overall_accuracy': accuracy,
            'correct_predictions': correct,
            'incorrect_predictions': total - correct,
            'accuracy_by_verdict': accuracy_by_verdict,
            'confusion_matrix': {k: dict(v) for k, v in confusion_matrix.items()}
        }

    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        metrics = self.calculate_accuracy()

        if 'error' in metrics:
            print(f"\nâœ— é”™è¯¯: {metrics['error']}")
            return

        print(f"\n{'=' * 80}")
        print(f"ğŸ“Š Verdictå‡†ç¡®åº¦æµ‹è¯•æŠ¥å‘Š")
        print(f"{'=' * 80}")

        print(f"\nã€åŸºæœ¬ä¿¡æ¯ã€‘")
        print(f"  æ€»æµ‹è¯•æ•°: {metrics['total_tests']}")
        print(f"  æˆåŠŸ: {metrics['successful_tests']}")
        print(f"  å¤±è´¥: {metrics['failed_tests']}")

        print(f"\nã€Verdictå‡†ç¡®åº¦ã€‘")
        print(f"  æ€»ä½“å‡†ç¡®ç‡: {metrics['overall_accuracy']:.2%}")
        print(f"  æ­£ç¡®é¢„æµ‹: {metrics['correct_predictions']}")
        print(f"  é”™è¯¯é¢„æµ‹: {metrics['incorrect_predictions']}")

        print(f"\nã€æŒ‰Verdictç±»åˆ«ç»Ÿè®¡ã€‘")
        for verdict, stats in metrics['accuracy_by_verdict'].items():
            print(f"  {verdict}:")
            print(f"    å‡†ç¡®ç‡: {stats['accuracy']:.2%}")
            print(f"    æ­£ç¡®/æ€»æ•°: {stats['correct']}/{stats['total']}")

        print(f"\nã€æ··æ·†çŸ©é˜µã€‘")
        verdicts = ['Supported', 'Refuted', 'Not Enough Evidence']
        cm = metrics['confusion_matrix']

        print(f"  {'Ground Truth':<25} | ", end='')
        for v in verdicts:
            print(f"{v[:10]:>10} ", end='')
        print()
        print(f"  {'-' * 25}-+-{'-' * 35}")

        for gt in verdicts:
            print(f"  {gt:<25} | ", end='')
            for pred in verdicts:
                count = cm.get(gt, {}).get(pred, 0)
                print(f"{count:>10} ", end='')
            print()

        print(f"\n{'=' * 80}")

    def save_results(self, output_path: str = 'verdict_test_results.json'):
        """ä¿å­˜å®Œæ•´ç»“æœä¸ºJSONæ ¼å¼"""
        output = {
            'metadata': {
                'model': 'qwen-plus-latest',
                'search_enabled': self.enable_search,
                'test_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_items': len(self.results)
            },
            'accuracy_metrics': self.calculate_accuracy(),
            'detailed_results': self.results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

        # è®¡ç®—æ–‡ä»¶å¤§å°
        import os
        file_size = os.path.getsize(output_path) / 1024  # KB
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} KB")

    def save_verdict_errors(self, output_path: str = 'verdict_errors.json'):
        """ä¿å­˜åˆ¤å†³é”™è¯¯çš„æ¡ˆä¾‹"""
        errors = []

        for r in self.results:
            if r['success'] and not r['verdict_evaluation']['is_correct']:
                errors.append({
                    'index': r['index'],
                    'claim': r['claim'],
                    'expected_verdict': r['verdict_evaluation']['expected'],
                    'predicted_verdict': r['verdict_evaluation']['predicted'],
                    'llm_justification': r['llm_response']['justification'],
                    'llm_confidence': r['llm_response']['confidence'],
                    'llm_evidence_sources': r['llm_response']['evidence_sources'],
                    'ground_truth_justification': r['ground_truth'].get('justification', ''),
                    'ground_truth_evidence_sources': r['ground_truth'].get('evidence_sources', [])
                })

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(errors, f, ensure_ascii=False, indent=2)

        print(f"âœ… åˆ¤å†³é”™è¯¯æ¡ˆä¾‹å·²ä¿å­˜åˆ°: {output_path}")
        print(f"   å…± {len(errors)} ä¸ªé”™è¯¯æ¡ˆä¾‹")


def main():
    # é…ç½®
    API_KEY = "sk-8faa7214041347609e67d5d09cec7266"
    DATASET_PATH = "data/dataset_part_8.json"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = VerdictTester(
        api_key=API_KEY,
        dataset_path=DATASET_PATH,
        enable_search=True  # å¼€å¯æœç´¢
    )

    # æµ‹è¯•æ•°æ®é›†
    print("å¼€å§‹æµ‹è¯•...\n")
    tester.test_dataset(max_items=100, start_index=0)  # å…ˆæµ‹è¯•3æ¡çœ‹æ•ˆæœ

    # æ‰“å°æ‘˜è¦
    tester.print_summary()

    # ä¿å­˜ç»“æœ
    tester.save_results('qwen3_max_max/verdict_test_results_8.json')

    # ä¿å­˜é”™è¯¯æ¡ˆä¾‹
    tester.save_verdict_errors('qwen3_max_max/verdict_errors_8.json')

    print("\n æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()