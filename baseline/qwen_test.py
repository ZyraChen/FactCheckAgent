"""
å¢å¼ºç‰ˆLLM Baselineæµ‹è¯•è„šæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰
åŒ…å«å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ï¼š
1. Verdict Accuracy
2. Evidence Macro-F1
3. Evidence Micro-F1
4. Explanation Correctness
5. ç»¼åˆScore

ä¸»è¦æ”¹è¿›ï¼š
1. æ›´æ˜ç¡®çš„promptï¼Œå¼ºåˆ¶LLMå¼•ç”¨è¯æ®æ¥æº
2. é™ä½æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼ä»0.3åˆ°0.2
3. æ”¹è¿›è¯æ®æå–é€»è¾‘ï¼Œæ”¯æŒæ›´å¤šå¼•ç”¨æ¨¡å¼
4. æ·»åŠ è¯¦ç»†çš„debugä¿¡æ¯ï¼Œæ˜¾ç¤ºè¯æ®åŒ¹é…è¿‡ç¨‹
"""

import json
import time
import re
from typing import List, Dict, Tuple, Set
from collections import defaultdict
import openai
from sklearn.metrics import f1_score, precision_recall_fscore_support


class QwenPlus:
    """é€šä¹‰åƒé—®LLMï¼ˆå¸¦æœç´¢åŠŸèƒ½ï¼‰"""

    def __init__(self, api_key):
        self.model = "qwen3-max"
        self.llm = openai.OpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

    def _cons_kwargs(self, messages: list[dict]) -> dict:
        kwargs = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.3,
            "timeout": 60,
        }
        return kwargs

    def completion(self, messages: list[dict], enable_thinking=False, return_json=False, enable_search=False) -> str:
        """
        è°ƒç”¨LLM completion

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            enable_thinking: æ˜¯å¦å¼€å¯æ€è€ƒæ¨¡å¼
            return_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
            enable_search: æ˜¯å¦å¼€å¯è”ç½‘æœç´¢åŠŸèƒ½
        """
        response_format = {"type": "json_object"} if not enable_thinking and return_json else {"type": "text"}
        extra_body = {}

        if enable_search:
            extra_body = {
                "enable_thinking": enable_thinking,
                "enable_search": True,
                "search_options": {
                    "forced_search": True
                }
            }

        try:
            rsp = self.llm.chat.completions.create(
                **self._cons_kwargs(messages),
                extra_body=extra_body if enable_search else None,
                response_format=response_format
            )
        except openai.RateLimitError as e:
            print("    âš ï¸  APIè¯·æ±‚è¶…è¿‡é™åˆ¶ï¼Œç­‰å¾…60ç§’...")
            time.sleep(60)
            rsp = self.llm.chat.completions.create(
                **self._cons_kwargs(messages),
                extra_body=extra_body if enable_search else None,
                response_format=response_format
            )
        except openai.APITimeoutError as e:
            print("    âš ï¸  APIè¯·æ±‚è¶…æ—¶ï¼Œç­‰å¾…60ç§’...")
            time.sleep(60)
            rsp = self.llm.chat.completions.create(
                **self._cons_kwargs(messages),
                extra_body=extra_body if enable_search else None,
                response_format=response_format
            )

        return rsp.choices[0].message.content


class EvidenceMatcher:
    """è¯æ®åŒ¹é…è¯„ä¼°å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""

    @staticmethod
    def extract_evidence_from_justification(justification: str, evidence_sources: List[Dict]) -> Set[str]:
        """
        ä»æ•°æ®é›†çš„evidence_sourcesä¸­æå–Ground Truthè¯æ®

        è¿™äº›å°±æ˜¯æ•°æ®é›†ä¸­æ ‡æ³¨çš„ã€ç”¨æ¥æ”¯æŒåˆ¤å†³çš„è¯æ®æ¥æº
        æ¯ä¸ªevidence_sourceåŒ…å«ï¼šcontentï¼ˆæ¥æºæè¿°ï¼‰ã€urlã€credibility

        è¿”å›ï¼šè¯æ®æ¥æºçš„é›†åˆï¼ˆä½¿ç”¨contentå­—æ®µä½œä¸ºæ ‡è¯†ï¼‰
        """
        referenced_sources = set()

        if not evidence_sources:
            return referenced_sources

        # å°†æ•°æ®é›†ä¸­çš„æ‰€æœ‰evidence_sourcesä½œä¸ºGround Truth
        for ev_source in evidence_sources:
            content = ev_source.get('content', '')
            url = ev_source.get('url', '')

            # ä¼˜å…ˆä½¿ç”¨contentï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨URL
            identifier = content if content else url
            if identifier:
                referenced_sources.add(identifier)

        return referenced_sources

    @staticmethod
    def extract_evidence_from_llm_reasoning(reasoning: str) -> Set[str]:
        """
        ä»LLMçš„reasoningä¸­æå–è¯æ®å¼•ç”¨

        LLMéœ€è¦åœ¨reasoningä¸­æ˜ç¡®å¼•ç”¨æ¥æºï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼æå–ï¼š
        1. URLé“¾æ¥
        2. å¼•ç”¨æ ¼å¼ï¼š"According to X"ã€"X reported"ç­‰
        3. æ•°å­—å’Œç™¾åˆ†æ¯”ï¼ˆç”¨äºåŒ¹é…ç»Ÿè®¡æ•°æ®ï¼‰

        è¿”å›ï¼šLLMå¼•ç”¨çš„è¯æ®æ¥æºé›†åˆ
        """
        llm_sources = set()

        if not reasoning:
            return llm_sources

        # 1. æå–URLï¼ˆæ›´å®½æ¾çš„æ­£åˆ™ï¼‰
        urls = re.findall(r'https?://[^\s\)\],]+', reasoning)
        llm_sources.update(urls)

        # 2. æå–å¼•ç”¨çš„æ¥æºåç§°ï¼ˆæ‰©å±•æ¨¡å¼ï¼‰
        citation_patterns = [
            # "According to X"
            r'[Aa]ccording to ([^,\.;]+)',
            # "X reported/stated/found..."
            r'([A-Z][a-zA-Z\s&\.]+(?:University|Institute|Bureau|Department|Bank|Agency|Organization|Commission|Post|Times|Journal|News|Guard|Press|Report|Survey|Index|Board))\s+(?:reported|stated|found|showed|indicated|confirmed|says|said)',
            # "Based on X"
            r'[Bb]ased on ([^,\.;]+(?:data|report|survey|study|analysis))',
            # "X's data/report"
            r"([A-Z][a-zA-Z\s&]+(?:University|Institute|Bureau|Department|Bank|Agency|Organization|Commission|Board))'s",
            # "The X report"
            r'[Tt]he ([A-Z][a-zA-Z\s&]+(?:Report|Survey|Index|Study))',
            # æ”¯æŒç¼©å†™ï¼š"According to FBI", "CDC data"
            r'[Aa]ccording to (?:the )?([A-Z]{2,})',
            r'([A-Z]{2,})\s+(?:data|report|survey|found|stated)',
        ]

        for pattern in citation_patterns:
            matches = re.findall(pattern, reasoning)
            for match in matches:
                clean_match = match.strip()
                if len(clean_match) > 2:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
                    llm_sources.add(clean_match)

        # 3. æå–æ•°å­—å’Œç™¾åˆ†æ¯”ï¼ˆç”¨äºå†…å®¹åŒ¹é…ï¼‰
        percentages = re.findall(r'\d+(?:\.\d+)?%', reasoning)
        llm_sources.update(percentages)

        # æå–è¾ƒå¤§çš„æ•°å­—
        large_numbers = re.findall(r'\b\d{1,3}(?:,\d{3})+\b', reasoning)
        llm_sources.update(large_numbers)

        return llm_sources

    @staticmethod
    def calculate_evidence_f1(ground_truth_sources: Set[str], predicted_sources: Set[str],
                             threshold: float = 0.2, debug: bool = False) -> Tuple[float, float, float, List[Tuple[str, str]]]:
        """
        è®¡ç®—è¯æ®åŒ¹é…çš„Precision, Recall, F1

        å‚æ•°ï¼š
            ground_truth_sources: æ•°æ®é›†ä¸­çš„evidence_sourcesï¼ˆGround Truthï¼‰
            predicted_sources: LLMåœ¨reasoningä¸­å¼•ç”¨çš„è¯æ®æ¥æº
            threshold: æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼ï¼ˆé™ä½åˆ°0.2ï¼Œæ›´å®¹æ˜“åŒ¹é…ï¼‰
            debug: æ˜¯å¦è¿”å›åŒ¹é…è¯¦æƒ…

        è®¡ç®—é€»è¾‘ï¼š
            1. å¯¹äºæ¯ä¸ªGTè¯æ®ï¼Œéå†æ‰€æœ‰LLMå¼•ç”¨çš„è¯æ®
            2. å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼ˆé€šè¿‡æ¨¡ç³ŠåŒ¹é…ï¼‰ï¼Œè®¡æ•°+1
            3. Precision = åŒ¹é…æ•° / LLMå¼•ç”¨æ€»æ•°
            4. Recall = åŒ¹é…æ•° / GTè¯æ®æ€»æ•°
            5. F1 = 2 * P * R / (P + R)

        è¿”å›ï¼š(precision, recall, f1, matched_pairs)
        """
        # ç‰¹æ®Šæƒ…å†µå¤„ç†
        if not ground_truth_sources and not predicted_sources:
            return 1.0, 1.0, 1.0, []

        if not ground_truth_sources or not predicted_sources:
            return 0.0, 0.0, 0.0, []

        # è®¡ç®—åŒ¹é…
        matched = 0
        matched_pairs = []

        for gt_src in ground_truth_sources:
            for pred_src in predicted_sources:
                # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
                if EvidenceMatcher._fuzzy_match(gt_src, pred_src, threshold):
                    matched += 1
                    matched_pairs.append((gt_src, pred_src))
                    break  # æ‰¾åˆ°åŒ¹é…åè·³å‡ºå†…å±‚å¾ªç¯

        # è®¡ç®—æŒ‡æ ‡
        precision = matched / len(predicted_sources) if predicted_sources else 0
        recall = matched / len(ground_truth_sources) if ground_truth_sources else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        return precision, recall, f1, matched_pairs

    @staticmethod
    def _fuzzy_match(text1: str, text2: str, threshold: float = 0.2) -> bool:
        """
        æ¨¡ç³ŠåŒ¹é…ä¸¤ä¸ªæ–‡æœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼Œé˜ˆå€¼é™ä½åˆ°0.2ï¼‰

        åŒ¹é…ç­–ç•¥ï¼š
        1. åŒ…å«å…³ç³»ï¼štext1 in text2 æˆ– text2 in text1
        2. ç²¾ç¡®åŒ¹é…ï¼šå®Œå…¨ç›¸åŒ
        3. URLåŸŸååŒ¹é…ï¼šå¦‚æœæ˜¯URLï¼Œæ¯”è¾ƒåŸŸå
        4. è¯é‡å ï¼šè®¡ç®—è¯çš„é‡å æ¯”ä¾‹ >= threshold

        å‚æ•°ï¼š
            threshold: è¯é‡å é˜ˆå€¼ï¼ˆé»˜è®¤0.2ï¼Œå³20%çš„è¯é‡å å°±ç®—åŒ¹é…ï¼‰
        """
        text1_lower = text1.lower()
        text2_lower = text2.lower()

        # æ–¹æ³•1: åŒ…å«å…³ç³»
        if text1_lower in text2_lower or text2_lower in text1_lower:
            return True

        # æ–¹æ³•2: ç²¾ç¡®åŒ¹é…
        if text1 == text2:
            return True

        # æ–¹æ³•3: URLåŸŸååŒ¹é…
        if 'http' in text1 or 'http' in text2:
            domain1 = re.findall(r'https?://([^/]+)', text1)
            domain2 = re.findall(r'https?://([^/]+)', text2)
            if domain1 and domain2 and domain1[0] == domain2[0]:
                return True

        # æ–¹æ³•4: è¯é‡å 
        words1 = set(re.findall(r'\w+', text1_lower))
        words2 = set(re.findall(r'\w+', text2_lower))

        # è¿‡æ»¤åœç”¨è¯ï¼ˆæ‰©å±•åˆ—è¡¨ï¼‰
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were',
            'according', 'reported', 'stated', 'said', 'based'
        }
        words1 = words1 - stopwords
        words2 = words2 - stopwords

        if not words1 or not words2:
            return False

        # è®¡ç®—è¯é‡å æ¯”ä¾‹
        overlap = len(words1 & words2)
        min_len = min(len(words1), len(words2))

        return (overlap / min_len) >= threshold if min_len > 0 else False


class ExplanationEvaluator:
    """è§£é‡Šè´¨é‡è¯„ä¼°å™¨"""

    def __init__(self, llm: QwenPlus):
        self.llm = llm

    def evaluate_explanation(self, claim: str, llm_reasoning: str, ground_truth_justification: str,
                           evidence_sources: List[Dict]) -> Dict:
        """
        è¯„ä¼°LLMè§£é‡Šçš„æ­£ç¡®æ€§
        è¿”å›è¯„ä¼°ç»“æœå­—å…¸
        """

        # æ„å»ºè¯„ä¼°prompt
        evidence_summary = "\n".join([
            f"- {ev.get('content', '')[:200]}..." for ev in evidence_sources[:3]
        ])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªäº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹LLMç”Ÿæˆçš„è§£é‡Šï¼ˆreasoningï¼‰æ˜¯å¦å­˜åœ¨ç¼ºé™·ã€‚

åŸå§‹å£°æ˜ï¼ˆClaimï¼‰:
{claim}

çœŸå®æ ‡æ³¨çš„è§£é‡Šï¼ˆGround Truth Justificationï¼‰:
{ground_truth_justification}

å¯ç”¨è¯æ®æ¥æº:
{evidence_summary}

LLMç”Ÿæˆçš„è§£é‡Šï¼ˆLLM Reasoningï¼‰:
{llm_reasoning}

è¯·ä»”ç»†æ£€æŸ¥LLMçš„è§£é‡Šæ˜¯å¦å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
1. **é€»è¾‘é”™è¯¯**: æ¨ç†è¿‡ç¨‹æ˜¯å¦æœ‰é€»è¾‘æ¼æ´æˆ–çŸ›ç›¾ï¼Ÿ
2. **è™šæ„å†…å®¹**: æ˜¯å¦ç¼–é€ äº†ä¸å­˜åœ¨çš„è¯æ®æˆ–äº‹å®ï¼Ÿ
3. **é”™è¯¯å¼•ç”¨**: æ˜¯å¦é”™è¯¯å¼•ç”¨æˆ–æ›²è§£äº†è¯æ®æ¥æºï¼Ÿ
4. **äº‹å®é”™è¯¯**: é™ˆè¿°çš„äº‹å®æ˜¯å¦æ­£ç¡®ï¼Ÿä¸çœŸå®æ ‡æ³¨ç›¸ç¬¦å—ï¼Ÿ
5. **çŸ¥è¯†é”™è¯¯**: æ˜¯å¦å¼•å…¥äº†é”™è¯¯çš„çŸ¥è¯†æˆ–æ¦‚å¿µï¼Ÿ
6. **æ¨ç†é”™è¯¯**: ä»è¯æ®åˆ°ç»“è®ºçš„æ¨ç†æ˜¯å¦åˆç†ï¼Ÿ

è¯„åˆ†æ ‡å‡†ï¼ˆ0-100åˆ†ï¼‰:
- 90-100: å®Œå…¨æ­£ç¡®ï¼Œæ— ä»»ä½•ç¼ºé™·
- 70-89: åŸºæœ¬æ­£ç¡®ï¼Œæœ‰è½»å¾®ç‘•ç–µ
- 50-69: éƒ¨åˆ†æ­£ç¡®ï¼Œæœ‰æ˜æ˜¾é—®é¢˜
- 30-49: ä¸¥é‡é”™è¯¯ï¼Œå¤šå¤„é—®é¢˜
- 0-29: å®Œå…¨é”™è¯¯ï¼Œä¸¥é‡è™šæ„

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼ˆä¸è¦ä½¿ç”¨markdownä»£ç å—ï¼‰:
{{
    "score": è¯„åˆ†ï¼ˆ0-100çš„æ•´æ•°ï¼‰,
    "has_logical_errors": true/false,
    "has_fabrication": true/false,
    "has_wrong_citation": true/false,
    "has_factual_errors": true/false,
    "has_knowledge_errors": true/false,
    "has_reasoning_errors": true/false,
    "explanation": "ç®€è¦è¯´æ˜å­˜åœ¨çš„ä¸»è¦é—®é¢˜ï¼ˆ2-3å¥è¯ï¼‰"
}}
"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm.completion(messages, return_json=True, enable_search=False)

            result = json.loads(response)

            # ç¡®ä¿scoreåœ¨0-100ä¹‹é—´
            result['score'] = max(0, min(100, result.get('score', 50)))

            return result

        except Exception as e:
            print(f"    âš ï¸  è§£é‡Šè¯„ä¼°å¤±è´¥: {e}")
            return {
                'score': 50,
                'has_logical_errors': False,
                'has_fabrication': False,
                'has_wrong_citation': False,
                'has_factual_errors': False,
                'has_knowledge_errors': False,
                'has_reasoning_errors': False,
                'explanation': f'è¯„ä¼°å¤±è´¥: {str(e)}'
            }


class EnhancedLLMFactChecker:
    """å¢å¼ºç‰ˆLLMäº‹å®æ ¸æŸ¥è¯„ä¼°å™¨"""

    def __init__(self, api_key: str, dataset_path: str, enable_search: bool = True, debug_mode: bool = False):
        self.llm = QwenPlus(api_key)
        self.dataset = self.load_dataset(dataset_path)
        self.results = []
        self.enable_search = enable_search
        self.evidence_matcher = EvidenceMatcher()
        self.explanation_evaluator = ExplanationEvaluator(self.llm)
        self.debug_mode = debug_mode  # æ˜¯å¦æ‰“å°è¯¦ç»†debugä¿¡æ¯

    def load_dataset(self, path: str) -> List[Dict]:
        """åŠ è½½æ•°æ®é›†"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def test_single_claim(self, item: Dict, index: int) -> Dict:
        """æµ‹è¯•å•ä¸ªclaim"""
        claim = item['claim']
        ground_truth_verdict = item['verdict']
        ground_truth_justification = item.get('justification', '')
        evidence_sources = item.get('evidence_sources', [])

        print(f"\n{'=' * 80}")
        print(f"[{index}] Claim: {claim[:100]}...")
        print(f"Ground Truth Verdict: {ground_truth_verdict}")

        try:
            # æ”¹è¿›çš„Prompt - å¼ºåˆ¶è¦æ±‚å¼•ç”¨è¯æ®
            prompt = f"""You are a professional fact-checker with access to search capabilities. Use web search to verify the following claim.

Claim: "{claim}"

CRITICAL REQUIREMENTS - You MUST follow these instructions:
1. Search for relevant, credible information about this claim
2. Carefully analyze the claim based on search results
3. In your reasoning, you MUST cite specific sources using these formats:
   - "According to [Source Name]..."
   - "[Organization/Publication] reported that..."
   - Include URLs when available: (https://...)
   - Cite specific numbers, dates, and statistics with their sources

4. Provide your verdict as one of these exact terms:
   - "Supported" (if the claim is true based on evidence)
   - "Refuted" (if the claim is false based on evidence)
   - "Not Enough Evidence" (if you cannot find sufficient information)

5. Your reasoning should be 4-6 sentences with EXPLICIT source citations

GOOD EXAMPLE:
{{
  "verdict": "Supported",
  "reasoning": "According to the Federal Reserve Bank of St. Louis (https://fred.stlouisfed.org/series/UMCSENT), the University of Michigan Consumer Sentiment Index fell by 13% from December 2024. The Conference Board also reported a 17% decline in their Consumer Confidence Index since November 2024, as stated in their official release (https://www.conference-board.org/...). These official statistics from government and industry sources confirm the claim about declining consumer confidence.",
  "confidence": "High"
}}

BAD EXAMPLE (DO NOT DO THIS):
{{
  "verdict": "Supported",
  "reasoning": "Consumer confidence has declined significantly based on recent economic data.",
  "confidence": "Medium"
}}

Respond ONLY with a valid JSON object:
{{
  "verdict": "...",
  "reasoning": "...",
  "confidence": "..."
}}

Do not include any text outside the JSON object."""

            messages = [{"role": "user", "content": prompt}]

            print(f"  ğŸ” æ­£åœ¨è®©LLMåˆ†æå¹¶æœç´¢è¯æ®...")
            response = self.llm.completion(messages, return_json=True, enable_search=self.enable_search)

            # è§£æå“åº”
            try:
                response_json = json.loads(response)
                llm_verdict = response_json.get('verdict', 'Not Enough Evidence')
                llm_reasoning = response_json.get('reasoning', '')
                llm_confidence = response_json.get('confidence', 'Unknown')
            except json.JSONDecodeError:
                print(f"    âš ï¸  JSONè§£æå¤±è´¥")
                llm_verdict = 'Not Enough Evidence'
                llm_reasoning = response
                llm_confidence = 'Low'

            print(f"  LLM Verdict: {llm_verdict}")
            print(f"  Confidence: {llm_confidence}")

            # 1. Verdict Accuracy
            verdict_match = (llm_verdict == ground_truth_verdict)
            print(f"  Verdict Match: {'âœ“' if verdict_match else 'âœ—'}")

            # 2. Evidence Matchingï¼ˆæ”¹è¿›ç‰ˆï¼‰
            print(f"\n  ğŸ“Š è¯„ä¼°è¯æ®åŒ¹é…åº¦...")

            # æå–Ground Truthè¯æ®ï¼ˆä»æ•°æ®é›†çš„evidence_sourceså­—æ®µï¼‰
            gt_evidence = self.evidence_matcher.extract_evidence_from_justification(
                ground_truth_justification, evidence_sources
            )

            # æå–LLM reasoningä¸­å¼•ç”¨çš„è¯æ®
            llm_evidence = self.evidence_matcher.extract_evidence_from_llm_reasoning(llm_reasoning)

            print(f"     Ground Truthè¯æ®æ•°ï¼ˆæ•°æ®é›†ä¸­çš„evidence_sourcesï¼‰: {len(gt_evidence)}")
            print(f"     LLMå¼•ç”¨è¯æ®æ•°ï¼ˆä»reasoningä¸­æå–ï¼‰: {len(llm_evidence)}")

            # Debugæ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†è¯æ®
            if self.debug_mode:
                print(f"\n     ã€Ground Truthè¯æ®è¯¦æƒ…ã€‘")
                for i, ev in enumerate(gt_evidence, 1):
                    print(f"       {i}. {ev[:100]}...")

                print(f"\n     ã€LLMå¼•ç”¨è¯æ®è¯¦æƒ…ã€‘")
                if llm_evidence:
                    for i, ev in enumerate(llm_evidence, 1):
                        print(f"       {i}. {ev}")
                else:
                    print(f"       âš ï¸  LLMæ²¡æœ‰å¼•ç”¨ä»»ä½•è¯æ®æ¥æºï¼")

            # è®¡ç®—Evidence F1ï¼ˆä½¿ç”¨é™ä½çš„é˜ˆå€¼0.2ï¼‰
            ev_precision, ev_recall, ev_f1, matched_pairs = self.evidence_matcher.calculate_evidence_f1(
                gt_evidence, llm_evidence, threshold=0.2, debug=True
            )

            print(f"     Evidence Precision: {ev_precision:.3f} (åŒ¹é…æ•°/LLMå¼•ç”¨æ•°)")
            print(f"     Evidence Recall: {ev_recall:.3f} (åŒ¹é…æ•°/GTè¯æ®æ•°)")
            print(f"     Evidence F1: {ev_f1:.3f}")

            # Debugæ¨¡å¼ï¼šæ˜¾ç¤ºåŒ¹é…è¯¦æƒ…
            if self.debug_mode and matched_pairs:
                print(f"\n     ã€åŒ¹é…çš„è¯æ®å¯¹ã€‘")
                for i, (gt, llm) in enumerate(matched_pairs, 1):
                    print(f"       {i}. GT:  {gt[:80]}...")
                    print(f"          LLM: {llm[:80]}...")

            # 3. Explanation Correctness
            print(f"\n  ğŸ” è¯„ä¼°è§£é‡Šæ­£ç¡®æ€§...")
            explanation_eval = self.explanation_evaluator.evaluate_explanation(
                claim, llm_reasoning, ground_truth_justification, evidence_sources
            )

            print(f"     Explanation Score: {explanation_eval['score']}/100")
            print(f"     ä¸»è¦é—®é¢˜: {explanation_eval.get('explanation', 'None')}")

            # 4. è®¡ç®—ç»¼åˆScore
            # Score = 0.4 * Verdict_Acc + 0.3 * Evidence_F1 + 0.3 * (Explanation_Score/100)
            verdict_score = 1.0 if verdict_match else 0.0
            evidence_score = ev_f1
            explanation_score = explanation_eval['score'] / 100.0

            overall_score = (0.4 * verdict_score +
                           0.3 * evidence_score +
                           0.3 * explanation_score)

            print(f"\n  ğŸ¯ ç»¼åˆå¾—åˆ†: {overall_score:.3f}")
            print(f"     = 0.4Ã—{verdict_score:.2f} + 0.3Ã—{evidence_score:.3f} + 0.3Ã—{explanation_score:.3f}")

            return {
                'index': index,
                'claim': claim,
                'ground_truth_verdict': ground_truth_verdict,
                'ground_truth_justification': ground_truth_justification,
                'ground_truth_evidence_count': len(evidence_sources),
                'ground_truth_evidence': list(gt_evidence),  # ä¿å­˜GTè¯æ®åˆ—è¡¨

                'llm_verdict': llm_verdict,
                'llm_reasoning': llm_reasoning,
                'llm_confidence': llm_confidence,
                'llm_evidence': list(llm_evidence),  # ä¿å­˜LLMå¼•ç”¨çš„è¯æ®åˆ—è¡¨

                # Metrics
                'verdict_match': verdict_match,
                'verdict_score': verdict_score,

                'evidence_precision': ev_precision,
                'evidence_recall': ev_recall,
                'evidence_f1': ev_f1,
                'evidence_score': evidence_score,
                'matched_evidence_pairs': [(gt[:100], llm[:100]) for gt, llm in matched_pairs],  # ä¿å­˜åŒ¹é…å¯¹

                'explanation_score': explanation_score,
                'explanation_eval': explanation_eval,

                'overall_score': overall_score,

                'success': True,
                'error': None,
                'original_data': item
            }

        except Exception as e:
            print(f"  âœ— Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return {
                'index': index,
                'claim': claim,
                'ground_truth_verdict': ground_truth_verdict,
                'success': False,
                'error': str(e),
                'original_data': item
            }

    def test_dataset(self, max_items: int = None, start_index: int = 0):
        """æµ‹è¯•æ•°æ®é›†"""
        print(f"{'=' * 80}")
        print(f"å¢å¼ºç‰ˆLLM Fact-Checking è¯„ä¼°")
        print(f"{'=' * 80}")
        print(f"æ•°æ®é›†å¤§å°: {len(self.dataset)}")
        print(f"LLMæ¨¡å‹: Qwen-3")
        print(f"æœç´¢åŠŸèƒ½: {'âœ… å·²å¼€å¯' if self.enable_search else 'âŒ æœªå¼€å¯'}")
        print(f"Debugæ¨¡å¼: {'âœ… å·²å¼€å¯' if self.debug_mode else 'âŒ æœªå¼€å¯'}")

        test_items = self.dataset[start_index:start_index + max_items] if max_items else self.dataset[start_index:]
        print(f"æµ‹è¯•æ•°é‡: {len(test_items)}\n")

        for i, item in enumerate(test_items, start=start_index):
            result = self.test_single_claim(item, i)
            self.results.append(result)

            # æ¯5ä¸ªæš‚åœä¸€ä¸‹
            if (i - start_index + 1) % 5 == 0:
                print(f"\nâ¸ï¸  å·²å¤„ç† {i - start_index + 1}/{len(test_items)}ï¼Œæš‚åœ3ç§’...")
                time.sleep(3)
            else:
                time.sleep(1)

        return self.results

    def calculate_metrics(self) -> Dict:
        """è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡"""
        successful = [r for r in self.results if r['success']]

        if not successful:
            return {'error': 'æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•'}

        total = len(successful)

        # 1. Verdict Accuracy
        verdict_accuracy = sum(r['verdict_match'] for r in successful) / total

        # 2. Evidence Macro-F1 (æŒ‰verdictåˆ†ç»„)
        by_verdict_evidence_f1 = defaultdict(list)
        for r in successful:
            verdict = r['ground_truth_verdict']
            by_verdict_evidence_f1[verdict].append(r['evidence_f1'])

        evidence_macro_f1_by_verdict = {
            verdict: sum(f1_list) / len(f1_list) if f1_list else 0.0
            for verdict, f1_list in by_verdict_evidence_f1.items()
        }

        # æ•´ä½“Evidence Macro-F1 (æ‰€æœ‰verdictçš„å¹³å‡)
        evidence_macro_f1 = sum(evidence_macro_f1_by_verdict.values()) / len(evidence_macro_f1_by_verdict) if evidence_macro_f1_by_verdict else 0.0

        # 3. Evidence Micro-F1 (å…¨å±€)
        evidence_micro_f1 = sum(r['evidence_f1'] for r in successful) / total

        # 4. Explanation Correctness
        avg_explanation_score = sum(r['explanation_score'] for r in successful) / total

        # ç»Ÿè®¡å„ç±»é”™è¯¯
        error_types = {
            'logical_errors': sum(r['explanation_eval'].get('has_logical_errors', False) for r in successful),
            'fabrication': sum(r['explanation_eval'].get('has_fabrication', False) for r in successful),
            'wrong_citation': sum(r['explanation_eval'].get('has_wrong_citation', False) for r in successful),
            'factual_errors': sum(r['explanation_eval'].get('has_factual_errors', False) for r in successful),
            'knowledge_errors': sum(r['explanation_eval'].get('has_knowledge_errors', False) for r in successful),
            'reasoning_errors': sum(r['explanation_eval'].get('has_reasoning_errors', False) for r in successful)
        }

        # 5. Overall Score
        overall_score = sum(r['overall_score'] for r in successful) / total

        # Verdictåˆ†ç»„å‡†ç¡®ç‡
        by_verdict_acc = defaultdict(lambda: {'total': 0, 'correct': 0})
        for r in successful:
            verdict = r['ground_truth_verdict']
            by_verdict_acc[verdict]['total'] += 1
            if r['verdict_match']:
                by_verdict_acc[verdict]['correct'] += 1

        verdict_accuracy_by_class = {
            verdict: stats['correct'] / stats['total'] if stats['total'] > 0 else 0.0
            for verdict, stats in by_verdict_acc.items()
        }

        # æ··æ·†çŸ©é˜µ
        confusion_matrix = defaultdict(lambda: defaultdict(int))
        for r in successful:
            gt = r['ground_truth_verdict']
            pred = r['llm_verdict']
            confusion_matrix[gt][pred] += 1

        return {
            'total_tests': len(self.results),
            'successful_tests': total,
            'failed_tests': len(self.results) - total,

            # ä¸»è¦æŒ‡æ ‡
            'verdict_accuracy': verdict_accuracy,
            'evidence_macro_f1': evidence_macro_f1,
            'evidence_micro_f1': evidence_micro_f1,
            'explanation_correctness': avg_explanation_score,
            'overall_score': overall_score,

            # è¯¦ç»†æŒ‡æ ‡
            'verdict_accuracy_by_class': verdict_accuracy_by_class,
            'evidence_macro_f1_by_verdict': evidence_macro_f1_by_verdict,
            'explanation_error_types': error_types,
            'confusion_matrix': {k: dict(v) for k, v in confusion_matrix.items()}
        }

    def print_report(self):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        metrics = self.calculate_metrics()

        if 'error' in metrics:
            print(f"\nâœ— é”™è¯¯: {metrics['error']}")
            return

        print(f"\n{'=' * 80}")
        print(f"ğŸ“Š å¢å¼ºç‰ˆäº‹å®æ ¸æŸ¥è¯„ä¼°æŠ¥å‘Š")
        print(f"{'=' * 80}")

        print(f"\nã€åŸºæœ¬ä¿¡æ¯ã€‘")
        print(f"  æ€»æµ‹è¯•æ•°: {metrics['total_tests']}")
        print(f"  æˆåŠŸ: {metrics['successful_tests']}")
        print(f"  å¤±è´¥: {metrics['failed_tests']}")

        print(f"\n{'=' * 80}")
        print(f"ã€æ ¸å¿ƒæŒ‡æ ‡ã€‘")
        print(f"{'=' * 80}")

        print(f"\n1ï¸âƒ£  Verdict Accuracy (åˆ¤å†³å‡†ç¡®åº¦)")
        print(f"   æ€»ä½“: {metrics['verdict_accuracy']:.2%}")
        print(f"   æŒ‰ç±»åˆ«:")
        for verdict, acc in metrics['verdict_accuracy_by_class'].items():
            print(f"     - {verdict}: {acc:.2%}")

        print(f"\n2ï¸âƒ£  Evidence Macro-F1 (è¯æ®åŒ¹é…-å®å¹³å‡)")
        print(f"   æ€»ä½“: {metrics['evidence_macro_f1']:.3f}")
        print(f"   æŒ‰åˆ¤å†³ç±»åˆ«:")
        for verdict, f1 in metrics['evidence_macro_f1_by_verdict'].items():
            print(f"     - {verdict}: {f1:.3f}")

        print(f"\n3ï¸âƒ£  Evidence Micro-F1 (è¯æ®åŒ¹é…-å¾®å¹³å‡)")
        print(f"   æ€»ä½“: {metrics['evidence_micro_f1']:.3f}")

        print(f"\n4ï¸âƒ£  Explanation Correctness (è§£é‡Šæ­£ç¡®æ€§)")
        print(f"   å¹³å‡å¾—åˆ†: {metrics['explanation_correctness']:.2%}")
        print(f"   é”™è¯¯ç±»å‹ç»Ÿè®¡:")
        for error_type, count in metrics['explanation_error_types'].items():
            pct = count / metrics['successful_tests'] * 100
            print(f"     - {error_type}: {count} ({pct:.1f}%)")

        print(f"\n5ï¸âƒ£  Overall Score (ç»¼åˆå¾—åˆ†)")
        print(f"   ç»¼åˆå¾—åˆ†: {metrics['overall_score']:.3f}")
        print(f"   è®¡ç®—å…¬å¼: 0.4Ã—Verdict_Acc + 0.3Ã—Evidence_F1 + 0.3Ã—Explanation")

        print(f"\n{'=' * 80}")
        print(f"ã€æ··æ·†çŸ©é˜µã€‘")
        print(f"{'=' * 80}")
        verdicts = ['Supported', 'Refuted', 'Not Enough Evidence']
        cm = metrics['confusion_matrix']

        # è¡¨å¤´
        print(f"   {'Ground Truth':<25} | ", end='')
        for v in verdicts:
            print(f"{v[:10]:>10} ", end='')
        print()
        print(f"   {'-' * 25}-+-{'-' * 35}")

        # æ•°æ®è¡Œ
        for gt in verdicts:
            print(f"   {gt:<25} | ", end='')
            for pred in verdicts:
                count = cm.get(gt, {}).get(pred, 0)
                print(f"{count:>10} ", end='')
            print()

        print(f"\n{'=' * 80}")

    def save_results(self, output_path: str = 'enhanced_test_results.json'):
        """ä¿å­˜å®Œæ•´ç»“æœ"""
        output = {
            'search_enabled': self.enable_search,
            'metrics': self.calculate_metrics(),
            'detailed_results': self.results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    def save_error_analysis(self, output_path: str = 'error_analysis.json'):
        """ä¿å­˜é”™è¯¯åˆ†æ"""
        errors = {
            'verdict_errors': [],
            'low_evidence_f1': [],
            'low_explanation_score': []
        }

        for r in self.results:
            if not r['success']:
                continue

            # Verdicté”™è¯¯
            if not r['verdict_match']:
                errors['verdict_errors'].append({
                    'index': r['index'],
                    'claim': r['claim'],
                    'ground_truth': r['ground_truth_verdict'],
                    'predicted': r['llm_verdict'],
                    'reasoning': r['llm_reasoning']
                })

            # Evidence F1ä½
            if r['evidence_f1'] < 0.3:
                errors['low_evidence_f1'].append({
                    'index': r['index'],
                    'claim': r['claim'],
                    'evidence_f1': r['evidence_f1'],
                    'ground_truth_evidence': r.get('ground_truth_evidence', []),
                    'llm_evidence': r.get('llm_evidence', []),
                    'matched_pairs': r.get('matched_evidence_pairs', []),
                    'ground_truth_justification': r['ground_truth_justification'],
                    'llm_reasoning': r['llm_reasoning']
                })

            # Explanationå¾—åˆ†ä½
            if r['explanation_score'] < 0.5:
                errors['low_explanation_score'].append({
                    'index': r['index'],
                    'claim': r['claim'],
                    'explanation_score': r['explanation_score'],
                    'explanation_eval': r['explanation_eval'],
                    'llm_reasoning': r['llm_reasoning']
                })

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(errors, f, ensure_ascii=False, indent=2)

        print(f"âœ… é”™è¯¯åˆ†æå·²ä¿å­˜åˆ°: {output_path}")
        print(f"   - Verdicté”™è¯¯: {len(errors['verdict_errors'])} æ¡")
        print(f"   - Evidence F1ä½: {len(errors['low_evidence_f1'])} æ¡")
        print(f"   - Explanationå¾—åˆ†ä½: {len(errors['low_explanation_score'])} æ¡")


def main():
    API_KEY = "sk-8faa7214041347609e67d5d09cec7266"
    DATASET_PATH = "../data/dataset_final.json"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„

    # åˆ›å»ºå¢å¼ºç‰ˆæµ‹è¯•å™¨
    tester = EnhancedLLMFactChecker(
        api_key=API_KEY,
        dataset_path=DATASET_PATH,
        enable_search=True,  # å¼€å¯æœç´¢
        debug_mode=True      # å¼€å¯debugæ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†è¯æ®åŒ¹é…è¿‡ç¨‹
    )

    # æµ‹è¯•æ•°æ®é›†
    tester.test_dataset(max_items=1000, start_index=0)  # å…ˆæµ‹è¯•3æ¡çœ‹æ•ˆæœ

    # æ‰“å°æŠ¥å‘Š
    tester.print_report()

    # ä¿å­˜ç»“æœ
    tester.save_results('enhanced_test_results_1_100.json')

    # ä¿å­˜é”™è¯¯åˆ†æ
    tester.save_error_analysis('error_analysis_1_100.json')


if __name__ == "__main__":
    main()